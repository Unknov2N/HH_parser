Порядок решения задачи
1. Парсинг страниц-вакансий HH по запросу пользователя, сохранение часто используемых слов в .json  
    а) вытаскивание ссылок со всех страниц по конкретному запросу _(прописан в коде)_ -- сделано через:  
    б) обновление частотного словаря по всем словам, найденным в ТЕКСТАХ вакансий, сохранение словаря в файл/БД  
    в) добавление аналогичного файла для поля "Ключевые навыки"  
2. Добавление external словаря для объединения слов (разные склонения, падежи) в одно [тут будет уместно научиться в SQL]  
    а) накопление словоформ (имеющихся слов различных падежей, склонений) для исходного _(нормализованного)_ слова с подсчётом их количества -- сделано  
    б) запись и подсчёт слов, следующих за исходным, "предсказание" -- сделано  
    в) написание скрипта для формирования вакансии через первое входное слово и предложения -- сделано   
    
минорные изменения  
	) создание "простого" файла с именами вакансий и их частотой; туда же добавить ЗП (min, max, avg)
    г) перенос данных о вакансиях в базу данных SQL-типа
    д) прогон вакансий.json>.txt через нейросеть via onigiri
 ~~а) создание функции поиска ключа(исходного слова) по введённому варианту  
   б) перечисление всех встреченных слов в значении ключа (ex. частота; слово; подобные → 544; высший; высшего, высшая, высшим, ...) [dict]  
   в) запись использованных рядом слов(левое_слово СЛОВО правое_слово) в порядке убывания их частоты (этакая попытка вытащить контекст) [вот тут и применим ООП]  
   г) объединение 2х слов в фразы при похожей частоте использования и присутствия в контексте друг друга  
   д) объединение 3х слов в фразы (больше нельзя, ибо у нас только (левое_слово СЛОВО правое_слово))~~